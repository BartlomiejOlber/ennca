# -*- coding: utf-8 -*-
"""Copy of template.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dIBcHEpqxPxY5GsiEfhTDAsNUUgiwftC

I run it on my own PC, so the parts handling GDrive are cut
"""

import os
from PIL import Image
import torch.utils.data as data
import torchvision.transforms as transforms


class ImageClassificationDataset(data.Dataset):
    def __init__(self, root, transform=None):
        self.root = root
        self.transform = transform

        self.classes = os.listdir(root)
        self.class_to_idx = {self.classes[i]: i for i in range(len(self.classes))}
        self.samples = self._make_dataset()

    def _make_dataset(self):
        images = []
        for target_class in os.listdir(self.root):
            target_dir = os.path.join(self.root, target_class)
            if not os.path.isdir(target_dir):
                continue
            for filename in os.listdir(target_dir):
                path = os.path.join(target_dir, filename)
                item = (path, self.class_to_idx[target_class])
                images.append(item)
        return images

    def __getitem__(self, index):
        path, label = self.samples[index]
        image = Image.open(path).convert('RGB')
        if self.transform is not None:
            image = self.transform(image)
        return image, label

    def __len__(self):
        return len(self.samples)


tf_train = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
     transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
     transforms.RandomRotation(20),
     transforms.RandomHorizontalFlip(p=0.5),
     ])

tf_val = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),
     ])

dataset_train = ImageClassificationDataset(root='./data/cifar10/cifar10/train', transform=tf_train)
dataset_val = ImageClassificationDataset(root='./data/cifar10/cifar10/test', transform=tf_val)

import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(6, 16, 5)
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = torch.flatten(x, 1)  # flatten all dimensions except batch
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x


from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning import loggers as pl_loggers
import torchvision.transforms as transforms
import numpy as np
import torch
import torch.nn as nn
import os
import json
import torchvision.models as models
import pytorch_lightning as pl
import random
import torchvision


class LightningEmotion(pl.LightningModule):

    def __init__(self, dicta):
        super(LightningEmotion, self).__init__()
        self.batch_size = dicta['bs']
        self.net = Net()
        self.loss = torch.nn.CrossEntropyLoss()
        self.validation_step_losses = []
        self.validation_step_accs = []

    def forward(self, x):
        out = self.net(x)
        return out

    def training_step(self, train_batch, batch_idx):
        x, y = train_batch
        logits = self.forward(x)
        preds = torch.argmax(self.forward(x), 1)
        acc_batch = ((preds == y).sum()) * 1.0 / x.shape[0]
        loss = self.loss(logits, y)

        logs = {'train_loss': loss, 'train_acc': acc_batch}
        return {'loss': loss, 'log': logs}

    def validation_step(self, val_batch, batch_idx):
        x, y = val_batch
        logits = self.forward(x)
        preds = torch.argmax(self.forward(x), 1)
        acc_batch = ((preds == y).sum()) * 1.0 / x.shape[0]
        loss = self.loss(logits, y)
        self.validation_step_losses.append(loss)
        self.validation_step_accs.append(acc_batch)
        return {'batch_val_loss': loss, 'batch_val_acc': acc_batch}

    def on_validation_epoch_end(self):
        avg_loss = torch.stack(self.validation_step_losses).mean()
        avg_acc = torch.stack(self.validation_step_accs).mean()
        tensorboard_logs = {'val_loss': avg_loss, 'val_acc': avg_acc}
        # print('validation acc:', avg_acc)
        self.log("val_loss", avg_loss)
        # return {'val_loss': avg_loss, 'val_acc': avg_acc, 'log': tensorboard_logs}

    def prepare_data(self):
        self.train_dataset = ImageClassificationDataset(root='./data/cifar10/cifar10/train',
                                                        transform=tf_train)
        self.val_dataset = ImageClassificationDataset(root='./data/cifar10/cifar10/test',
                                                      transform=tf_val)

    def train_dataloader(self):
        return torch.utils.data.DataLoader(
            self.train_dataset, batch_size=self.batch_size, num_workers=2, shuffle=True)

    def val_dataloader(self):
        return torch.utils.data.DataLoader(
            self.val_dataset, batch_size=self.batch_size, num_workers=2, shuffle=False)

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)
        return {'optimizer': optimizer, 'lr_scheduler': scheduler, 'monitor': 'val_loss'}


# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir '/content/drive/MyDrive/ennca/record'

import pytorch_lightning

early_stop_callback = pytorch_lightning.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0.0,
    patience=20,
    verbose=False,
    mode='min'
)

checkpoint_callback = ModelCheckpoint(
    filename='train',
    save_top_k=1,
    verbose=True,
    monitor='val_loss',
    mode='min',
)

tb_logger = pl_loggers.TensorBoardLogger('./record', name='1st')
dicta = {'bs': 256}
model = LightningEmotion(dicta)
trainer = pl.Trainer(
    callbacks=[early_stop_callback, checkpoint_callback], max_epochs=300,
    logger=tb_logger)
trainer.fit(model)
