{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "NJ9Tbd62GBjQ",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9b9d4751-d143-4f04-cef3-cb88fca8c185"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1oHoYT7J4-xKfNu6cfBOMKHyO0QBqdYsI\n",
      "To: /content/calibration_data2.zip\n",
      "100% 134M/134M [00:02<00:00, 51.9MB/s]\n",
      "Archive:  /content/calibration_data2.zip\n",
      "replace /content/calibration_data/data/ILSVRC2012_val_00000001.JPEG? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!gdown 1oHoYT7J4-xKfNu6cfBOMKHyO0QBqdYsI\n",
    "!unzip /content/calibration_data2.zip -d /content/calibration_data"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!ls /content/\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eh6ywzz3SSTG",
    "outputId": "6487d471-6d92-48d4-9cb9-47b2015ed73f"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "calibration_data  calibration_data2.zip  sample_data\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.quantization.qconfig import QConfig\n",
    "from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, MovingAveragePerChannelMinMaxObserver, PerChannelMinMaxObserver\n",
    "import yaml\n",
    "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.ao.quantization.quantize_fx as quantize_fx\n",
    "import copy"
   ],
   "metadata": {
    "id": "C6s0M5cqGFOU"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "  example_inputs = (torch.randn(1, 3, 224, 224),)\n",
    "  model_fp = torchvision.models.resnet18(pretrained=True)\n",
    "  model_to_quantize = copy.deepcopy(model_fp)\n",
    "  quantize_fx.fuse_fx(model_to_quantize.eval())\n",
    "  transform_cali = transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "      transforms.Normalize(\n",
    "          mean=[0.485, 0.456, 0.406],  # ImageNet dataset mean\n",
    "          std=[0.229, 0.224, 0.225]  # ImageNet dataset standard deviation\n",
    "      )\n",
    "  ])\n",
    "  transform_val = transforms.Compose([\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
    "      transforms.Normalize(\n",
    "          mean=[0.485, 0.456, 0.406],  # ImageNet dataset mean\n",
    "          std=[0.229, 0.224, 0.225]  # ImageNet dataset standard deviation\n",
    "      )\n",
    "  ])\n"
   ],
   "metadata": {
    "id": "JApUvwubGy8y",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "03618e3d-c4a3-456c-f5f1-7f13642900c6"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import os\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, txt_file, transform=None, dir=None):\n",
    "        self.data = []\n",
    "        with open(txt_file, 'r') as f:\n",
    "            for line in f:\n",
    "                image_path = line.split(' ')[0]\n",
    "                label = line.split(' ')[1].split('\\n')[0]\n",
    "                self.data.append((image_path, int(label)))\n",
    "        self.transform = transform\n",
    "        self.dir = dir\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.data[idx]\n",
    "        if self.dir:\n",
    "            image_path = os.path.join(self.dir, image_path)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, original_dataset, samples_per_epoch):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.samples_per_epoch = samples_per_epoch\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        random_index = random.randint(0, len(self.original_dataset) - 1)\n",
    "        return self.original_dataset[random_index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.samples_per_epoch"
   ],
   "metadata": {
    "id": "V96u5oWgL5SP"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "val_dataset = MyDataset('/content/calibration_data/samples.txt',\n",
    "                        transform=transform_val)"
   ],
   "metadata": {
    "id": "Lg8pVJv8L9wa"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(val_dataset))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dRruHhhASi5O",
    "outputId": "e0c2a007-83f6-42d5-816b-bc4fb9a03cca"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "896\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig = QConfig(activation=FakeQuantize.with_args(observer=MinMaxObserver,\n",
    "                                                          quant_min=0,\n",
    "                                                          quant_max=255,\n",
    "                                                          qscheme=torch.per_tensor_affine,\n",
    "                                                          reduce_range=False,),\n",
    "                        weight=FakeQuantize.with_args(observer=PerChannelMinMaxObserver,\n",
    "                                                      quant_min=-128,\n",
    "                                                      quant_max=127,\n",
    "                                                      dtype=torch.qint8,\n",
    "                                                      qscheme=torch.per_channel_symmetric,\n",
    "                                                      #per_tensor_symmetric\n",
    "                                                      reduce_range=False,\n",
    "                                                      ch_axis=0,\n",
    "                                                        ))"
   ],
   "metadata": {
    "id": "wmm5k0PSIeDX"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.ao.quantization import (\n",
    "  get_default_qconfig_mapping,\n",
    "  get_default_qat_qconfig_mapping,\n",
    "  QConfigMapping,\n",
    ")\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)"
   ],
   "metadata": {
    "id": "GxDDltbQIVn7"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)"
   ],
   "metadata": {
    "id": "2P29ygKSIb80"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ],
   "metadata": {
    "id": "ZTc5w0GNM2xS"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, validation_dataloader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    avg_loss = running_loss / len(validation_dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ],
   "metadata": {
    "id": "CnvDYm45M0xP"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False, num_workers=8, drop_last=True)"
   ],
   "metadata": {
    "id": "BVhlXRWhNM7O",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "acc9e147-9c35-4638-9a29-e16d9f9362ff"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc = evaluate_model(model_prepared, val_loader, criterion,'cuda')"
   ],
   "metadata": {
    "id": "01znXw1-Qprr"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "acc"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_qLIhKywRr-L",
    "outputId": "6651cbce-2f24-4062-dd9c-355b6deb85cb"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "69.75"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_prepared = model_prepared.to('cpu')"
   ],
   "metadata": {
    "id": "L63wdI2tRnIl"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_quantized = quantize_fx.convert_fx(model_prepared)"
   ],
   "metadata": {
    "id": "Krn6CuqrRFDa"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc = evaluate_model(model_quantized, val_loader, criterion,'cpu')"
   ],
   "metadata": {
    "id": "726p1s-2RHG1"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "acc"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8tjInGJRt_N",
    "outputId": "b69e030e-3d57-4bfa-dcac-ff61dea85f64"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "66.625"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_prepared"
   ],
   "metadata": {
    "id": "I0tBh2pUTJY4",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "f5b7a3d7-8932-45c7-fa65-e7c38d03cefd"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GraphModule(\n",
       "  (activation_post_process_0): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0187]), zero_point=tensor([114], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-2.1179039478302, max_val=2.640000104904175)\n",
       "  )\n",
       "  (conv1): ConvReLU2d(\n",
       "    (0): QuantizedConv2d(Reference)(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): ReLU(inplace=True)\n",
       "  )\n",
       "  (activation_post_process_1): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0273]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=6.968907356262207)\n",
       "  )\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (activation_post_process_2): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0273]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=6.968907356262207)\n",
       "  )\n",
       "  (layer1): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): QuantizedConv2d(Reference)(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): QuantizedConv2d(Reference)(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): QuantizedConv2d(Reference)(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): QuantizedConv2d(Reference)(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_3): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0213]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=5.433541297912598)\n",
       "  )\n",
       "  (activation_post_process_4): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0517]), zero_point=tensor([141], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-7.298849105834961, max_val=5.877784729003906)\n",
       "  )\n",
       "  (activation_post_process_5): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0290]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=7.3822922706604)\n",
       "  )\n",
       "  (activation_post_process_6): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0171]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=4.36132287979126)\n",
       "  )\n",
       "  (activation_post_process_7): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0610]), zero_point=tensor([155], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-9.433801651000977, max_val=6.116794586181641)\n",
       "  )\n",
       "  (activation_post_process_8): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0349]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=8.910597801208496)\n",
       "  )\n",
       "  (layer2): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): QuantizedConv2d(Reference)(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): QuantizedConv2d(Reference)(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(Reference)(64, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): QuantizedConv2d(Reference)(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): QuantizedConv2d(Reference)(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_9): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0166]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=4.24070930480957)\n",
       "  )\n",
       "  (activation_post_process_10): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0441]), zero_point=tensor([95], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-4.207176685333252, max_val=7.047486305236816)\n",
       "  )\n",
       "  (activation_post_process_11): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0343]), zero_point=tensor([135], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-4.641353607177734, max_val=4.094722270965576)\n",
       "  )\n",
       "  (activation_post_process_12): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0283]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=7.211561679840088)\n",
       "  )\n",
       "  (activation_post_process_13): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0199]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=5.0794172286987305)\n",
       "  )\n",
       "  (activation_post_process_14): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0434]), zero_point=tensor([141], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-6.102829933166504, max_val=4.972873687744141)\n",
       "  )\n",
       "  (activation_post_process_15): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0286]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=7.2954630851745605)\n",
       "  )\n",
       "  (layer3): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): QuantizedConv2d(Reference)(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): QuantizedConv2d(Reference)(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(Reference)(128, 256, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): QuantizedConv2d(Reference)(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): QuantizedConv2d(Reference)(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_16): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0150]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=3.836390972137451)\n",
       "  )\n",
       "  (activation_post_process_17): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0406]), zero_point=tensor([92], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-3.7141711711883545, max_val=6.627476215362549)\n",
       "  )\n",
       "  (activation_post_process_18): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0167]), zero_point=tensor([169], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-2.8219733238220215, max_val=1.4427917003631592)\n",
       "  )\n",
       "  (activation_post_process_19): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0267]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=6.810975551605225)\n",
       "  )\n",
       "  (activation_post_process_20): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0194]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=4.936844825744629)\n",
       "  )\n",
       "  (activation_post_process_21): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0503]), zero_point=tensor([154], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-7.742077827453613, max_val=5.073171615600586)\n",
       "  )\n",
       "  (activation_post_process_22): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0300]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=7.6541876792907715)\n",
       "  )\n",
       "  (layer4): Module(\n",
       "    (0): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): QuantizedConv2d(Reference)(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): QuantizedConv2d(Reference)(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (downsample): Module(\n",
       "        (0): QuantizedConv2d(Reference)(256, 512, kernel_size=(1, 1), stride=(2, 2))\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Module(\n",
       "      (conv1): ConvReLU2d(\n",
       "        (0): QuantizedConv2d(Reference)(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): QuantizedConv2d(Reference)(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (activation_post_process_23): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0141]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=3.5952749252319336)\n",
       "  )\n",
       "  (activation_post_process_24): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0431]), zero_point=tensor([111], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-4.772782802581787, max_val=6.208411693572998)\n",
       "  )\n",
       "  (activation_post_process_25): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0266]), zero_point=tensor([121], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-3.2209134101867676, max_val=3.5685651302337646)\n",
       "  )\n",
       "  (activation_post_process_26): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0314]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=8.008794784545898)\n",
       "  )\n",
       "  (activation_post_process_27): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.0128]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=3.257014274597168)\n",
       "  )\n",
       "  (activation_post_process_28): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2951]), zero_point=tensor([57], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-16.894067764282227, max_val=58.34833908081055)\n",
       "  )\n",
       "  (activation_post_process_29): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2493]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=63.574275970458984)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (activation_post_process_30): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2493]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=63.574275970458984)\n",
       "  )\n",
       "  (activation_post_process_31): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.2493]), zero_point=tensor([0], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=0.0, max_val=63.574275970458984)\n",
       "  )\n",
       "  (fc): QuantizedLinear(Reference)(in_features=512, out_features=1000, bias=True)\n",
       "  (activation_post_process_32): FakeQuantize(\n",
       "    fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, ch_axis=-1, scale=tensor([0.1720]), zero_point=tensor([65], dtype=torch.int32)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-11.161824226379395, max_val=32.69382858276367)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#TASK:\n",
    "1. Split the 1000 samples into two groups, 50%,50%, one of them will be named calibration_dataset while the other will be validation_dataset. You should use calibration dataset to calibrate scale and zero point before performing actual quantization.\n",
    "2. Make PTQ training for MobileNetV2, using PerchannelMinMax Quantization for weights and PerTensorMinMax Quantization separately.\n",
    "3. Make Perchannel MovingAverageMinMax Quantization for weights and MovingAverage Pertensor Quantization for activation. Compare to results in 2., Check which one is better. Compare some scales and zero points of from the observers. Explain why one of the solution is better?"
   ],
   "metadata": {
    "id": "J71DR1VHSKEG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import copy\n",
    "import random\n",
    "# val_dataset = MyDataset('samples.txt', transform=transform_val)\n",
    "calibration_dataset = copy.deepcopy(val_dataset)\n",
    "random.seed(2137)\n",
    "random.shuffle(calibration_dataset.data)\n",
    "validation_dataset = copy.deepcopy(calibration_dataset)\n",
    "validation_dataset.data = validation_dataset.data[:len(validation_dataset)//2]\n",
    "calibration_dataset.data = calibration_dataset.data[len(calibration_dataset)//2:]\n",
    "calibration_loader = DataLoader(calibration_dataset, batch_size=100, shuffle=False, num_workers=8, drop_last=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=100, shuffle=False, num_workers=8, drop_last=True)\n"
   ],
   "metadata": {
    "id": "Ew80NoYZcM0t",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3a4c5e38-2163-4e8a-c004-ec0c4e5947f2"
   },
   "execution_count": 49,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(validation_dataset), len(calibration_dataset))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alTf88avdZqZ",
    "outputId": "745d82d8-1642-4721-b4ba-736592c2d1fa"
   },
   "execution_count": 50,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "448 448\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "quantize_fx.fuse_fx(model_to_quantize.eval())\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
    "loss, acc = evaluate_model(model_prepared, calibration_loader, criterion,'cuda')"
   ],
   "metadata": {
    "id": "A0R1uUkNcVck"
   },
   "execution_count": 51,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_prepared = model_prepared.to('cpu')\n",
    "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
    "loss, acc = evaluate_model(model_prepared, validation_loader, criterion,'cuda')"
   ],
   "metadata": {
    "id": "aOqVLzt4dJe5"
   },
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"calibrated:\", loss, acc)\n",
    "# calibration seems to improve the accuracy\n",
    "# lets compare it with no quantization"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OC3fDf7cfZKJ",
    "outputId": "6e1b9f70-19ac-468d-eca7-a53bce3f3ddb"
   },
   "execution_count": 53,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "calibrated: 1.3138143718242645 66.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "quantize_fx.fuse_fx(model_to_quantize.eval())\n",
    "loss, acc = evaluate_model(model_to_quantize, validation_loader, criterion,'cuda')"
   ],
   "metadata": {
    "id": "jKLLW2AlgeD9"
   },
   "execution_count": 54,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"no quantization (fused)\", loss, acc)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "asRK4i7BgHDy",
    "outputId": "703aa9e0-a8a7-41a5-db67-9dcfdd41d044"
   },
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "no quantization (fused) 1.1694874167442322 69.75\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MobileNetV2"
   ],
   "metadata": {
    "id": "s_T_dNO5inqA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "  \n",
    "model_fp = torchvision.models.quantization.mobilenet_v2(pretrained=True)\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "quantize_fx.fuse_fx(model_to_quantize.eval())\n"
   ],
   "metadata": {
    "id": "b5XKNU73gtv1"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig = QConfig(activation=FakeQuantize.with_args(observer=MinMaxObserver,\n",
    "                                                          quant_min=0,\n",
    "                                                          quant_max=255,\n",
    "                                                          qscheme=torch.per_tensor_affine,\n",
    "                                                          reduce_range=False,),\n",
    "                        weight=FakeQuantize.with_args(observer=PerChannelMinMaxObserver,\n",
    "                                                      quant_min=-128,\n",
    "                                                      quant_max=127,\n",
    "                                                      dtype=torch.qint8,\n",
    "                                                      qscheme=torch.per_channel_symmetric,\n",
    "                                                      #per_tensor_symmetric\n",
    "                                                      reduce_range=False,\n",
    "                                                      ch_axis=0,\n",
    "                                                        ))\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)"
   ],
   "metadata": {
    "id": "kV9ajo40h8dO"
   },
   "execution_count": 57,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc = evaluate_model(model_prepared, calibration_loader, criterion,'cuda')"
   ],
   "metadata": {
    "id": "2RAFRCyciapF"
   },
   "execution_count": 58,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_prepared = model_prepared.to('cpu')\n",
    "model_quantized_per_channel_minmax = quantize_fx.convert_fx(model_prepared)"
   ],
   "metadata": {
    "id": "IvkZkbBojgnm"
   },
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc = evaluate_model(model_quantized_per_channel_minmax, validation_loader, criterion,'cpu')"
   ],
   "metadata": {
    "id": "1vJCPyMajhHV"
   },
   "execution_count": 60,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"per channel observer: \", loss, acc)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MMQJSlWTjzpz",
    "outputId": "6deabcd4-8595-43be-e65e-55a82f60c026"
   },
   "execution_count": 61,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "per channel observer:  1.7089119255542755 58.5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig = QConfig(activation=FakeQuantize.with_args(observer=MinMaxObserver,\n",
    "                                                          quant_min=0,\n",
    "                                                          quant_max=255,\n",
    "                                                          qscheme=torch.per_tensor_affine,\n",
    "                                                          reduce_range=False,),\n",
    "                        weight=FakeQuantize.with_args(observer=MinMaxObserver,\n",
    "                                                      quant_min=-128,\n",
    "                                                      quant_max=127,\n",
    "                                                      dtype=torch.qint8,\n",
    "                                                      qscheme=torch.per_tensor_symmetric,\n",
    "                                                      reduce_range=False,\n",
    "                                                        ))\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "quantize_fx.fuse_fx(model_to_quantize.eval())\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)"
   ],
   "metadata": {
    "id": "xoXXucl0TVSn"
   },
   "execution_count": 62,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc = evaluate_model(model_prepared, calibration_loader, criterion,'cuda')\n",
    "model_prepared = model_prepared.to('cpu')\n",
    "model_quantized_minmax_per_tensor = quantize_fx.convert_fx(model_prepared)\n",
    "loss, acc = evaluate_model(model_quantized_minmax_per_tensor, validation_loader, criterion,'cpu')\n",
    "print(\"per tensor observer: \", loss, acc)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1xFupy_GV1xX",
    "outputId": "5b007955-5acf-4c8b-fba5-6e84e59e7b81"
   },
   "execution_count": 63,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "per tensor observer:  1.3971586227416992 66.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig = QConfig(activation=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver,\n",
    "                                                          quant_min=0,\n",
    "                                                          quant_max=255,\n",
    "                                                          qscheme=torch.per_tensor_affine,\n",
    "                                                          reduce_range=False,),\n",
    "                        weight=FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver,\n",
    "                                                      quant_min=-128,\n",
    "                                                      quant_max=127,\n",
    "                                                      dtype=torch.qint8,\n",
    "                                                      qscheme=torch.per_channel_symmetric,\n",
    "                                                      reduce_range=False,\n",
    "                                                      ch_axis=0,\n",
    "                                                        ))\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "quantize_fx.fuse_fx(model_to_quantize.eval())\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)"
   ],
   "metadata": {
    "id": "634NsY7rZKgW"
   },
   "execution_count": 64,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc = evaluate_model(model_prepared, calibration_loader, criterion,'cuda')\n",
    "model_prepared = model_prepared.to('cpu')\n",
    "model_quantized_moving_average_per_channel = quantize_fx.convert_fx(model_prepared)\n",
    "loss, acc = evaluate_model(model_quantized_moving_average_per_channel, validation_loader, criterion,'cpu')\n",
    "print(\"moving average per channel observer: \", loss, acc)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nODDn16QaQut",
    "outputId": "0efa4e05-d304-4636-c09d-59949694835b"
   },
   "execution_count": 65,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "moving average per channel observer:  1.6461626291275024 60.5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "qconfig = QConfig(activation=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver,\n",
    "                                                          quant_min=0,\n",
    "                                                          quant_max=255,\n",
    "                                                          qscheme=torch.per_tensor_affine,\n",
    "                                                          reduce_range=False,),\n",
    "                        weight=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver,\n",
    "                                                      quant_min=-128,\n",
    "                                                      quant_max=127,\n",
    "                                                      dtype=torch.qint8,\n",
    "                                                      qscheme=torch.per_tensor_affine,\n",
    "                                                      reduce_range=False,\n",
    "                                                        ))\n",
    "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
    "model_to_quantize = copy.deepcopy(model_fp)\n",
    "quantize_fx.fuse_fx(model_to_quantize.eval())\n",
    "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)"
   ],
   "metadata": {
    "id": "-I0QmdjrsIPZ"
   },
   "execution_count": 66,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "loss, acc = evaluate_model(model_prepared, calibration_loader, criterion,'cuda')\n",
    "model_prepared = model_prepared.to('cpu')\n",
    "model_quantized_moving_average_per_tensor = quantize_fx.convert_fx(model_prepared)\n",
    "loss, acc = evaluate_model(model_quantized_moving_average_per_tensor, validation_loader, criterion,'cpu')\n",
    "print(\"moving average per tensor (affine scheme) observer: \", loss, acc)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YHjJ8k8gwCrq",
    "outputId": "5f4fcad8-9184-4b85-8951-beb4655b5c7d"
   },
   "execution_count": 67,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "moving average per tensor (affine scheme) observer:  1.4038624167442322 66.5\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Moving average observers yield better results in this case (I tried with a different seed and the conclusions were likewise). Per tensor moving average minmax,for this seed, is better than standard per tensor minmax by 0.5pp and per channel moving average is better than standard per channel by 2pp. \n",
    "I guess that's because moving average calibration tends to \"ignore\" the extreme Xmin and Xmax values, which results in a smaller [Xmin; Xmax] interval which results in a smaller scale parameter. Smaller scale parameter means denser quantization so the results tend to be more accurate."
   ],
   "metadata": {
    "id": "DsKKtClLwXaH"
   }
  }
 ]
}
