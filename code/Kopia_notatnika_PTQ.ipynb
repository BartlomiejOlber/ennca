{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ9Tbd62GBjQ"
      },
      "outputs": [],
      "source": [
        "!gdown 1oHoYT7J4-xKfNu6cfBOMKHyO0QBqdYsI\n",
        "!unzip /content/calibration_data2.zip -d /content/calibration_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh6ywzz3SSTG",
        "outputId": "cec74f3e-92fd-40d5-d70b-adc76eb091b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.quantization.qconfig import QConfig\n",
        "from torch.quantization.observer import MinMaxObserver, MovingAverageMinMaxObserver, MovingAveragePerChannelMinMaxObserver, PerChannelMinMaxObserver\n",
        "import yaml\n",
        "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.ao.quantization.quantize_fx as quantize_fx\n",
        "import copy"
      ],
      "metadata": {
        "id": "C6s0M5cqGFOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  example_inputs = (torch.randn(1, 3, 224, 224),)\n",
        "  model_fp = torchvision.models.resnet18(pretrained=True)\n",
        "  model_to_quantize = copy.deepcopy(model_fp)\n",
        "  quantize_fx.fuse_fx(model_to_quantize.eval())\n",
        "  transform_cali = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "      transforms.Normalize(\n",
        "          mean=[0.485, 0.456, 0.406],  # ImageNet dataset mean\n",
        "          std=[0.229, 0.224, 0.225]  # ImageNet dataset standard deviation\n",
        "      )\n",
        "  ])\n",
        "  transform_val = transforms.Compose([\n",
        "      transforms.Resize(256),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "      transforms.Normalize(\n",
        "          mean=[0.485, 0.456, 0.406],  # ImageNet dataset mean\n",
        "          std=[0.229, 0.224, 0.225]  # ImageNet dataset standard deviation\n",
        "      )\n",
        "  ])\n"
      ],
      "metadata": {
        "id": "JApUvwubGy8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de074b0-b8ca-49a9-dbb6-16623be6d1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import random\n",
        "import os\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, txt_file, transform=None, dir=None):\n",
        "        self.data = []\n",
        "        with open(txt_file, 'r') as f:\n",
        "            for line in f:\n",
        "                image_path = line.split(' ')[0]\n",
        "                label = line.split(' ')[1].split('\\n')[0]\n",
        "                self.data.append((image_path, int(label)))\n",
        "        self.transform = transform\n",
        "        self.dir = dir\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.data[idx]\n",
        "        if self.dir:\n",
        "            image_path = os.path.join(self.dir, image_path)\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, original_dataset, samples_per_epoch):\n",
        "        self.original_dataset = original_dataset\n",
        "        self.samples_per_epoch = samples_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        random_index = random.randint(0, len(self.original_dataset) - 1)\n",
        "        return self.original_dataset[random_index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.samples_per_epoch"
      ],
      "metadata": {
        "id": "V96u5oWgL5SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = MyDataset('/content/calibration_data/samples.txt',\n",
        "                        transform=transform_val)"
      ],
      "metadata": {
        "id": "Lg8pVJv8L9wa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRruHhhASi5O",
        "outputId": "b8a30da6-ba35-438d-91a3-a7a01d7c6b5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qconfig = QConfig(activation=FakeQuantize.with_args(observer=MinMaxObserver,\n",
        "                                                          quant_min=0,\n",
        "                                                          quant_max=255,\n",
        "                                                          qscheme=torch.per_tensor_affine,\n",
        "                                                          reduce_range=False,),\n",
        "                        weight=FakeQuantize.with_args(observer=PerChannelMinMaxObserver,\n",
        "                                                      quant_min=-128,\n",
        "                                                      quant_max=127,\n",
        "                                                      dtype=torch.qint8,\n",
        "                                                      qscheme=torch.per_channel_symmetric,\n",
        "                                                      #per_tensor_symmetric\n",
        "                                                      reduce_range=False,\n",
        "                                                      ch_axis=0,\n",
        "                                                        ))"
      ],
      "metadata": {
        "id": "wmm5k0PSIeDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.ao.quantization import (\n",
        "  get_default_qconfig_mapping,\n",
        "  get_default_qat_qconfig_mapping,\n",
        "  QConfigMapping,\n",
        ")\n",
        "qconfig_mapping = QConfigMapping().set_global(qconfig)"
      ],
      "metadata": {
        "id": "GxDDltbQIVn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)"
      ],
      "metadata": {
        "id": "2P29ygKSIb80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "ZTc5w0GNM2xS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, validation_dataloader, criterion, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in validation_dataloader:\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = running_loss / len(validation_dataloader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ],
      "metadata": {
        "id": "CnvDYm45M0xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(val_dataset, batch_size=100, shuffle=False, num_workers=8, drop_last=True)"
      ],
      "metadata": {
        "id": "BVhlXRWhNM7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a871613-c11a-4c92-e51c-c89baabd98e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = evaluate_model(model_prepared, val_loader, criterion,'cuda')"
      ],
      "metadata": {
        "id": "01znXw1-Qprr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qLIhKywRr-L",
        "outputId": "b6967024-5061-41d3-a91c-d60d01f606de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "69.75"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_prepared = model_prepared.to('cpu')"
      ],
      "metadata": {
        "id": "L63wdI2tRnIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ],
      "metadata": {
        "id": "Krn6CuqrRFDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = evaluate_model(model_quantized, val_loader, criterion,'cpu')"
      ],
      "metadata": {
        "id": "726p1s-2RHG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8tjInGJRt_N",
        "outputId": "0c989057-6361-41d1-8e82-c099be05101b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.625"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_prepared"
      ],
      "metadata": {
        "id": "I0tBh2pUTJY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TASK:\n",
        "1. Split the 1000 samples into two groups, 50%,50%, one of them will be named calibration_dataset while the other will be validation_dataset. You should use calibration dataset to calibrate scale and zero point before performing actual quantization.\n",
        "2. Make PTQ training for MobileNetV2, using PerchannelMinMax Quantization for weights and PerTensorMinMax Quantization separately.\n",
        "3. Make Perchannel MovingAverageMinMax Quantization for weights and MovingAverage Pertensor Quantization for activation. Compare to results in 2., Check which one is better. Compare some scales and zero points of from the observers. Explain why one of the solution is better?"
      ],
      "metadata": {
        "id": "J71DR1VHSKEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import random\n",
        "# val_dataset = MyDataset('samples.txt', transform=transform_val)\n",
        "calibration_dataset = copy.deepcopy(val_dataset)\n",
        "random.shuffle(calibration_dataset.data)\n",
        "validation_dataset = copy.deepcopy(calibration_dataset)\n",
        "validation_dataset.data = validation_dataset.data[:len(validation_dataset)//2]\n",
        "calibration_dataset.data = calibration_dataset.data[len(calibration_dataset)//2:]\n",
        "calibration_loader = DataLoader(calibration_dataset, batch_size=100, shuffle=False, num_workers=8, drop_last=True)\n",
        "validation_loader = DataLoader(validation_dataset, batch_size=100, shuffle=False, num_workers=8, drop_last=True)\n"
      ],
      "metadata": {
        "id": "Ew80NoYZcM0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(validation_dataset), len(calibration_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alTf88avdZqZ",
        "outputId": "cf6e451c-120f-4d0f-bec1-63025fea242c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "448 448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)\n",
        "loss, acc = evaluate_model(model_prepared, calibration_loader, criterion,'cuda')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0R1uUkNcVck",
        "outputId": "6485dcc8-14c5-4b2d-ef36-68a24a6a5e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss, acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_8sloMNdNxi",
        "outputId": "d20c1d09-1794-43c3-aa03-2f7b1e847a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.343494862318039 69.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_prepared = model_prepared.to('cpu')\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)\n",
        "loss, acc = evaluate_model(model_prepared, validation_loader, criterion,'cuda')"
      ],
      "metadata": {
        "id": "aOqVLzt4dJe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss, acc)\n",
        "# with a quantization with a separate calibration dataset it really seems to work better\n",
        "# lets compare it with no quantization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC3fDf7cfZKJ",
        "outputId": "e3803bbc-87e9-4a8a-a0c8-147a84bac08f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2071868181228638 71.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = evaluate_model(model_fp, validation_loader, criterion,'cuda')"
      ],
      "metadata": {
        "id": "jKLLW2AlgeD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss, acc)\n",
        "# funny, quantization improves the results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asRK4i7BgHDy",
        "outputId": "773e8ad4-af02-4652-f8ba-91edc61bb217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2008384466171265 70.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MobileNetV2"
      ],
      "metadata": {
        "id": "s_T_dNO5inqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "model_fp = torchvision.models.quantization.mobilenet_v2(pretrained=True)\n",
        "model_to_quantize = copy.deepcopy(model_fp)\n",
        "quantize_fx.fuse_fx(model_to_quantize.eval())\n"
      ],
      "metadata": {
        "id": "b5XKNU73gtv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qconfig = QConfig(activation=FakeQuantize.with_args(observer=MinMaxObserver,\n",
        "                                                          quant_min=0,\n",
        "                                                          quant_max=255,\n",
        "                                                          qscheme=torch.per_tensor_affine,\n",
        "                                                          reduce_range=False,),\n",
        "                        weight=FakeQuantize.with_args(observer=PerChannelMinMaxObserver,\n",
        "                                                      quant_min=-128,\n",
        "                                                      quant_max=127,\n",
        "                                                      dtype=torch.qint8,\n",
        "                                                      qscheme=torch.per_channel_symmetric,\n",
        "                                                      #per_tensor_symmetric\n",
        "                                                      reduce_range=False,\n",
        "                                                      ch_axis=0,\n",
        "                                                        ))\n",
        "qconfig_mapping = QConfigMapping().set_global(qconfig)\n",
        "model_prepared = quantize_fx.prepare_fx(model_to_quantize, qconfig_mapping, example_inputs)"
      ],
      "metadata": {
        "id": "kV9ajo40h8dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = evaluate_model(model_prepared, calibration_loader, criterion,'cuda')"
      ],
      "metadata": {
        "id": "2RAFRCyciapF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss, acc)\n",
        "model_prepared = model_prepared.to('cpu')\n",
        "model_quantized = quantize_fx.convert_fx(model_prepared)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvkZkbBojgnm",
        "outputId": "c823542b-c860-45a2-d99c-6c6a2b444f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.3331595063209534 66.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, acc = evaluate_model(model_prepared, validation_loader, criterion,'cuda')"
      ],
      "metadata": {
        "id": "1vJCPyMajhHV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss, acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMQJSlWTjzpz",
        "outputId": "ca37c4d8-91c4-4343-e9f7-20e6cefa1a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1564173102378845 72.5\n"
          ]
        }
      ]
    }
  ]
}